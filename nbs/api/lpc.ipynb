{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LPC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp lpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from functools import partial\n",
    "from typing import Tuple\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import lineax as lx\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "from jax.scipy.linalg import toeplitz\n",
    "from jax.typing import ArrayLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# for testing purposes\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def next_power_of_2(x):\n",
    "    return 1 if x == 0 else 2 ** (x - 1).bit_length()\n",
    "\n",
    "\n",
    "def autocorr(\n",
    "    x: ArrayLike,\n",
    "    p: int,\n",
    "    biased: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the autocorrelation of a signal x up to lag p.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x: ArrayLike\n",
    "        The input signal to compute the autocorrelation of.\n",
    "    p: int\n",
    "        The maximum lag to compute the autocorrelation up to.\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.abs(np.fft.rfft(x, next_power_of_2(x.shape[-1] + 1)))\n",
    "    R = np.fft.irfft(X**2)[..., : p + 1]\n",
    "    if biased:\n",
    "        R = R / x.shape[-1]  # Biased autocorrelation\n",
    "    return R\n",
    "\n",
    "\n",
    "def unfold(\n",
    "    x: ArrayLike,  # (batch, time, channels)\n",
    "    kernel_size: int,\n",
    "    stride: int = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Unfolds a signal x into a matrix of patches of size kernel_size and stride\n",
    "    similar to the pytorch unfold function.\n",
    "    \"\"\"\n",
    "    # unfold the y signal\n",
    "    unfolded = jax.lax.conv_general_dilated_patches(\n",
    "        x,\n",
    "        filter_shape=(kernel_size,),\n",
    "        window_strides=(stride,),\n",
    "        padding=\"VALID\",\n",
    "        dimension_numbers=(\"NTC\", \"OIT\", \"NTC\"),\n",
    "    )\n",
    "    return unfolded\n",
    "\n",
    "\n",
    "def unfold_np(x, kernel_size, stride=1):\n",
    "    \"\"\"\n",
    "    Unfolds a signal x (batch, time, channels) into a matrix of patches of size kernel_size and stride,\n",
    "    similar to the pytorch unfold function, using numpy's as_strided function.\n",
    "    \"\"\"\n",
    "    num_patches = (len(x) - kernel_size) // stride + 1\n",
    "\n",
    "    new_shape = (num_patches, kernel_size)\n",
    "    new_strides = (x.strides[0] * stride, x.strides[0])\n",
    "\n",
    "    return np.lib.stride_tricks.as_strided(x, shape=new_shape, strides=new_strides)\n",
    "\n",
    "\n",
    "def covariance_matrix(\n",
    "    x: ArrayLike,  # (batch, time)\n",
    "    p: int,\n",
    "    L: int = None,\n",
    "):\n",
    "    if L is None:\n",
    "        L = x.shape[-1]\n",
    "\n",
    "    # We need a (B, T, C) shape, the unfold is done on the T dimension\n",
    "    # we also need to flip the first dimension to get the autocorrelation\n",
    "    return jnp.flip(unfold(x[..., None], L - p), axis=1)\n",
    "\n",
    "\n",
    "def covariance_matrix_np(\n",
    "    x: np.ndarray,  # (time,)\n",
    "    p: int,\n",
    "    L: int = None,\n",
    "):\n",
    "    if L is None:\n",
    "        L = x.shape[-1]\n",
    "\n",
    "    # We need a (B, T, C) shape, the unfold is done on the T dimension\n",
    "    # we also need to flip the first dimension to get the autocorrelation\n",
    "    return np.flip(unfold_np(x, L - p), axis=0)\n",
    "\n",
    "\n",
    "def discrete_convolution(\n",
    "    x: np.ndarray,  # (time,)\n",
    "    a: np.ndarray,  # (time, p)\n",
    "    zi=None,  # (p,)\n",
    "):\n",
    "    \"\"\"\n",
    "    Convolves a signal x with a time-varying filter defined by a.\n",
    "    \"\"\"\n",
    "\n",
    "    if zi is None:\n",
    "        zi = np.zeros(a.shape[-1])\n",
    "    x_padded = np.concatenate([zi, x])\n",
    "    e_hat_conv = np.zeros_like(x)\n",
    "\n",
    "    for n in range(len(x)):\n",
    "        e_hat_conv[n] = x_padded[n + a.shape[-1]] + np.dot(\n",
    "            a[n, ::-1], x_padded[n : n + a.shape[-1]]\n",
    "        )\n",
    "\n",
    "    return e_hat_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "x = np.arange(10).astype(np.float64)\n",
    "unfolded_jax = unfold(x[None, :, None], 3)\n",
    "unfolded_np = unfold_np(x, 3)\n",
    "\n",
    "X_np = covariance_matrix_np(x, 3, len(x))\n",
    "X_jax = covariance_matrix(x[None, :], 3, len(x))\n",
    "\n",
    "assert np.allclose(X_np, X_jax[0])\n",
    "assert np.allclose(unfolded_np, unfolded_jax[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"p\",))\n",
    "def jitted_lstsq(x, p):\n",
    "    T = x.shape[-1]\n",
    "    corr = jnp.correlate(x, x, mode=\"full\")[T - 1 : T + p] / T\n",
    "\n",
    "    a = toeplitz(corr[:-1])\n",
    "    b = corr[1:]\n",
    "    sol_lstsq, *_ = jnp.linalg.lstsq(a, b, rcond=None)\n",
    "    return sol_lstsq\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"p\",))\n",
    "def jitted_linear_solve(x, p):\n",
    "    T = x.shape[-1]\n",
    "    corr = jnp.correlate(x, x, mode=\"full\")[T - 1 : T + p] / T\n",
    "\n",
    "    a = toeplitz(corr[:-1])\n",
    "    b = corr[1:]\n",
    "\n",
    "    solver = lx.LU()\n",
    "    return lx.linear_solve(lx.MatrixLinearOperator(a), b, solver).value\n",
    "\n",
    "\n",
    "def lpc_cpu_solve(\n",
    "    x: np.ndarray,  # (time,)\n",
    "    p: int,  # order\n",
    "    method=\"autocorrelation\",  # method for the lpc computation\n",
    "    **kwargs,  # additional arguments for the method\n",
    ") -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Compute the Linear Prediction Coefficients (LPC) of a signal x.\n",
    "    Based on \"Introduction to Digital Speech Processing\" by Rabiner and Schafer.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    x: np.ndarray\n",
    "        The input signal.\n",
    "    p: int\n",
    "        The order of the LPC.\n",
    "    method: str\n",
    "        The method to compute the LPC. Either \"autocorrelation\" or \"covariance\".\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The LPC coefficients.\n",
    "    int\n",
    "        The gain (or the square root of the energy) of the residual signal.\n",
    "\n",
    "    \"\"\"\n",
    "    if method == \"autocorrelation\":\n",
    "        # biased autocorrelation\n",
    "        R = autocorr(x, p, **kwargs).astype(\n",
    "            np.float64\n",
    "        )  # make sure it's float64 for numerical stability\n",
    "\n",
    "        # uses levinson-durbin recursion\n",
    "        a = scipy.linalg.solve_toeplitz(R[:-1], -R[1:])\n",
    "        g = np.sqrt(R[0] + a @ R[1:])\n",
    "        return a, g\n",
    "\n",
    "    elif method == \"covariance\":\n",
    "        covar_mat = covariance_matrix_np(x, p, len(x)).astype(\n",
    "            np.float64\n",
    "        )  # make sure it's float64 for numerical stability\n",
    "        R = covar_mat @ covar_mat.T\n",
    "\n",
    "        # uses cholesky decomposition\n",
    "        # c, low = cho_factor(R, lower=False)\n",
    "        # a = cho_solve((c, low), r_0)\n",
    "        a = scipy.linalg.solve(R[1:, 1:], -R[0, 1:])\n",
    "        g = np.sqrt(R[0, 0] + a @ R[0, 1:])\n",
    "        return a, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03891267  0.05897309 -0.11110881 -0.10518435] 8.848879174571616\n",
      "[-0.05351606  0.04524858 -0.09969181 -0.10755192] 8.666443074154966\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(100)\n",
    "\n",
    "p = 4\n",
    "a_autocorr, g_autocorr = lpc_cpu_solve(x, p, method=\"autocorrelation\", biased=False)\n",
    "a_covar, g_covar = lpc_cpu_solve(x, p, method=\"covariance\")\n",
    "\n",
    "print(a_autocorr, g_autocorr)\n",
    "print(a_covar, g_covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def linear_prediction_np(\n",
    "    x: np.ndarray,  # (T,)\n",
    "    a: np.ndarray,  # (p,)\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the linear prediction of a 1D signal. The signal will be padded on the left with zeros.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    x: ArrayLike\n",
    "        The signal used to compute the linear prediction\n",
    "    a: ArrayLike\n",
    "        The coefficients of the linear prediction\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    ArrayLike:\n",
    "        The linear prediction of the signal of the same shape as x\n",
    "\n",
    "    \"\"\"\n",
    "    p = a.shape[-1]\n",
    "    return np.convolve(np.pad(x[:-1], (p, 0)), a, mode=\"valid\")\n",
    "\n",
    "\n",
    "def linear_prediction(\n",
    "    x: ArrayLike,  # (T)\n",
    "    a: ArrayLike,  # (p)\n",
    ") -> ArrayLike:\n",
    "    \"\"\"\n",
    "    Computes the linear prediction of a 1D signal. The signal will be padded on the left with zeros.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    x: ArrayLike\n",
    "        The signal used to compute the linear prediction\n",
    "    a: ArrayLike\n",
    "        The coefficients of the linear prediction\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    ArrayLike:\n",
    "        The linear prediction of the signal of the same shape as x\n",
    "\n",
    "    \"\"\"\n",
    "    p = a.shape[-1]\n",
    "    return jnp.convolve(\n",
    "        jnp.pad(x[:-1], (p, 0)),\n",
    "        a,\n",
    "        mode=\"valid\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | test\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "x = rng.normal(0, 1, 100)\n",
    "a = rng.normal(0, 1, 4)\n",
    "\n",
    "lp_np = linear_prediction_np(x, a)\n",
    "lp_jax = linear_prediction(x, a)\n",
    "\n",
    "assert np.allclose(lp_np, lp_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def inverse_filter_np(\n",
    "    x: np.ndarray,  # (T,)\n",
    "    a: np.ndarray,  # (p,)\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the inverse filter of a signal x using the coefficients a.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    x: ArrayLike\n",
    "        The signal used to compute the inverse filter\n",
    "    a: ArrayLike\n",
    "        The coefficients of the linear prediction\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    ArrayLike:\n",
    "        The inverse filter of the signal of the same shape as x\n",
    "    \"\"\"\n",
    "\n",
    "    e_hat = scipy.signal.lfilter(\n",
    "        b=np.concatenate([[1], a]),\n",
    "        a=[1],\n",
    "        x=x,\n",
    "    )\n",
    "    return e_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def coeffs_and_residual(\n",
    "    y: ArrayLike,  # input signal (time,)\n",
    "    p: int,  # number of coefficients\n",
    "    **kwargs,  # additional arguments\n",
    ") -> Tuple[ArrayLike, ArrayLike, float]:  # coefficients (p+1) and residual and gain\n",
    "    \"\"\"\n",
    "    Utility function to compute the LPC coefficients and the residual.\n",
    "    \"\"\"\n",
    "    if np.all(y == 0):\n",
    "        a = np.zeros((p + 1))\n",
    "        a[0] = 1\n",
    "        e_hat = np.zeros_like(y)\n",
    "        return a, e_hat, 0\n",
    "\n",
    "    a, g = lpc_cpu_solve(y, p, **kwargs)\n",
    "    e_hat = inverse_filter_np(y, a)  # Apply filter to each slice-frame\n",
    "\n",
    "    return a, e_hat, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "x = rng.normal(size=1000)\n",
    "pole = np.array([0.999 * np.exp(1j * np.pi / 4)])\n",
    "poles = np.concatenate([pole, np.conj(pole)])\n",
    "b, a = scipy.signal.zpk2tf([], poles, 1)\n",
    "y = scipy.signal.lfilter(b, a, x)\n",
    "\n",
    "p = 10\n",
    "a_autocorr, e_hat_autocorr, _ = coeffs_and_residual(x, p, method=\"autocorrelation\")\n",
    "a_covar, e_hat_covar, _ = coeffs_and_residual(x, p, method=\"covariance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
