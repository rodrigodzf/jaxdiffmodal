{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of the CQT1992v2 from [nnAudio](https://github.com/KinWaiCheuk/nnAudio) in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp feautures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "import warnings\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import lax\n",
    "from scipy.signal import get_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def nextpow2(A):\n",
    "    return int(np.ceil(np.log2(A)))\n",
    "\n",
    "\n",
    "def broadcast_dim(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x[:, None, :]\n",
    "    elif x.ndim == 1:\n",
    "        x = x[None, None, :]\n",
    "    elif x.ndim == 3:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Only support input with shape = (batch, len) or shape = (len)\"\n",
    "        )\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_window_dispatch(window, N, fftbins=True):\n",
    "    \"\"\"Get the window function.\"\"\"\n",
    "    if isinstance(window, str):\n",
    "        return get_window(window, N, fftbins=fftbins)\n",
    "    elif isinstance(window, tuple):\n",
    "        if window[0] == \"gaussian\":\n",
    "            assert window[1] >= 0\n",
    "            sigma = np.floor(-N / 2 / np.sqrt(-2 * np.log(10 ** (-window[1] / 20))))\n",
    "            return get_window((\"gaussian\", sigma), N, fftbins=fftbins)\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                \"Tuple windows may have undesired behaviour regarding Q factor\"\n",
    "            )\n",
    "            return get_window(window, N, fftbins=fftbins)\n",
    "    elif isinstance(window, float):\n",
    "        warnings.warn(\n",
    "            \"You are using Kaiser window with beta factor \"\n",
    "            + str(window)\n",
    "            + \". Correct behaviour not checked.\"\n",
    "        )\n",
    "        return get_window(window, N, fftbins=fftbins)\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"The function get_window from scipy only supports strings, tuples and floats.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def create_cqt_kernels(\n",
    "    Q,\n",
    "    fs,\n",
    "    fmin,\n",
    "    n_bins=84,\n",
    "    bins_per_octave=12,\n",
    "    norm=1,\n",
    "    window=\"hann\",\n",
    "    fmax=None,\n",
    "    topbin_check=True,\n",
    "    gamma=0,\n",
    "):\n",
    "    \"\"\"Create CQT kernels in time domain.\"\"\"\n",
    "\n",
    "    if (fmax is not None) and (n_bins is None):\n",
    "        n_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n",
    "        freqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.double(bins_per_octave))\n",
    "    elif (fmax is None) and (n_bins is not None):\n",
    "        freqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.double(bins_per_octave))\n",
    "    else:\n",
    "        warnings.warn(\"If fmax is given, n_bins will be ignored\", SyntaxWarning)\n",
    "        n_bins = np.ceil(bins_per_octave * np.log2(fmax / fmin))\n",
    "        freqs = fmin * 2.0 ** (np.r_[0:n_bins] / np.double(bins_per_octave))\n",
    "\n",
    "    if np.max(freqs) > fs / 2 and topbin_check:\n",
    "        raise ValueError(\n",
    "            f\"The top bin {np.max(freqs)}Hz has exceeded the Nyquist frequency, \"\n",
    "            \"please reduce the n_bins\"\n",
    "        )\n",
    "\n",
    "    alpha = 2.0 ** (1.0 / bins_per_octave) - 1.0\n",
    "    lengths = np.ceil(Q * fs / (freqs + gamma / alpha))\n",
    "\n",
    "    # get max window length depending on gamma value\n",
    "    max_len = int(max(lengths))\n",
    "    fftLen = int(2 ** (np.ceil(np.log2(max_len))))\n",
    "\n",
    "    tempKernel = np.zeros((int(n_bins), int(fftLen)), dtype=np.complex64)\n",
    "\n",
    "    for k in range(0, int(n_bins)):\n",
    "        freq = freqs[k]\n",
    "        l = int(lengths[k])\n",
    "\n",
    "        # Centering the kernels\n",
    "        if l % 2 == 1:  # pad more zeros on RHS\n",
    "            start = int(np.ceil(fftLen / 2.0 - l / 2.0)) - 1\n",
    "        else:\n",
    "            start = int(np.ceil(fftLen / 2.0 - l / 2.0))\n",
    "\n",
    "        window_func = get_window_dispatch(window, l, fftbins=True)\n",
    "        sig = (\n",
    "            window_func\n",
    "            * np.exp(np.r_[-l // 2 : l // 2] * 1j * 2 * np.pi * freq / fs)\n",
    "            / l\n",
    "        )\n",
    "\n",
    "        if norm:  # Normalizing the filter\n",
    "            tempKernel[k, start : start + l] = sig / np.linalg.norm(sig, norm)\n",
    "        else:\n",
    "            tempKernel[k, start : start + l] = sig\n",
    "\n",
    "    return tempKernel, fftLen, jnp.array(lengths, dtype=jnp.float32), freqs\n",
    "\n",
    "\n",
    "# More efficient implementation using JAX's lax.conv_general_dilated\n",
    "def conv1d_efficient(x, kernel, stride=1):\n",
    "    # Get dimensions\n",
    "    batch_size, channels_in, width = x.shape\n",
    "    channels_out, kernel_channels, kernel_width = kernel.shape\n",
    "\n",
    "    # Ensure kernel channels match input channels\n",
    "    assert channels_in == kernel_channels, (\n",
    "        f\"Input channels ({channels_in}) must match kernel channels ({kernel_channels})\"\n",
    "    )\n",
    "\n",
    "    # Reshape for JAX's conv_general_dilated\n",
    "    # Move channel dimension for proper convolution\n",
    "    x = x.transpose(0, 2, 1)  # [batch, width, channels_in]\n",
    "\n",
    "    # Reshape kernel: [out_channels, in_channels, kernel_width] -> [kernel_width, in_channels, out_channels]\n",
    "    kernel = kernel.transpose(2, 1, 0)  # [kernel_width, in_channels, out_channels]\n",
    "\n",
    "    # Define dimension numbers for 1D convolution\n",
    "    dimension_numbers = lax.ConvDimensionNumbers(\n",
    "        lhs_spec=(0, 2, 1),  # batch, features, spatial dims\n",
    "        rhs_spec=(2, 1, 0),  # output features, input features, spatial dims\n",
    "        out_spec=(0, 2, 1),  # batch, features, spatial dims\n",
    "    )\n",
    "\n",
    "    # Perform convolution\n",
    "    output = lax.conv_general_dilated(\n",
    "        x,  # input\n",
    "        kernel,  # kernel\n",
    "        (stride,),  # stride\n",
    "        \"VALID\",  # padding\n",
    "        dimension_numbers=dimension_numbers,\n",
    "    )\n",
    "\n",
    "    # Transpose back to match expected output format [batch, channels, width]\n",
    "    output = output.transpose(0, 2, 1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class CQT1992v2:\n",
    "    \"\"\"JAX implementation of CQT1992v2 from nnAudio.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sr=22050,\n",
    "        hop_length=512,\n",
    "        fmin=32.70,\n",
    "        fmax=None,\n",
    "        n_bins=84,\n",
    "        bins_per_octave=12,\n",
    "        filter_scale=1,\n",
    "        norm=1,\n",
    "        window=\"hann\",\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        trainable=False,\n",
    "        output_format=\"Magnitude\",\n",
    "    ):\n",
    "        self.hop_length = hop_length\n",
    "        self.center = center\n",
    "        self.pad_mode = pad_mode\n",
    "        self.output_format = output_format\n",
    "        self.trainable = trainable\n",
    "\n",
    "        # Creating kernels for CQT\n",
    "        Q = float(filter_scale) / (2 ** (1 / bins_per_octave) - 1)\n",
    "\n",
    "        print(\"Creating CQT kernels...\")\n",
    "        cqt_kernels, self.kernel_width, self.lengths, self.frequencies = (\n",
    "            create_cqt_kernels(Q, sr, fmin, n_bins, bins_per_octave, norm, window, fmax)\n",
    "        )\n",
    "\n",
    "        # Convert to JAX arrays\n",
    "        self.cqt_kernels_real = jnp.array(cqt_kernels.real)[:, None, :]\n",
    "        self.cqt_kernels_imag = jnp.array(cqt_kernels.imag)[:, None, :]\n",
    "        print(\"CQT kernels created\")\n",
    "\n",
    "    def __call__(self, x, output_format=None, normalization_type=\"librosa\"):\n",
    "        \"\"\"Forward pass of the CQT transform.\"\"\"\n",
    "        return self.forward(x, output_format, normalization_type)\n",
    "\n",
    "    def forward(self, x, output_format=None, normalization_type=\"librosa\"):\n",
    "        \"\"\"\n",
    "        Convert a batch of waveforms to CQT spectrograms.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : jax.numpy.ndarray\n",
    "            Input signal should be in either of the following shapes:\n",
    "            1. (len_audio)\n",
    "            2. (num_audio, len_audio)\n",
    "            3. (num_audio, 1, len_audio)\n",
    "\n",
    "        normalization_type : str\n",
    "            Type of the normalisation. Options:\n",
    "            'librosa' : output fits the librosa implementation\n",
    "            'convolutional' : output conserves the convolutional inequalities\n",
    "            'wrap' : wraps positive and negative frequencies into positive frequencies\n",
    "        \"\"\"\n",
    "        output_format = output_format or self.output_format\n",
    "\n",
    "        # Convert numpy arrays to JAX arrays if needed\n",
    "        if not isinstance(x, jnp.ndarray):\n",
    "            x = jnp.array(x)\n",
    "\n",
    "        # Broadcast dimensions\n",
    "        x = broadcast_dim(x)\n",
    "\n",
    "        # Apply padding if center is True\n",
    "        if self.center:\n",
    "            pad_width = self.kernel_width // 2\n",
    "            if self.pad_mode == \"constant\":\n",
    "                x = jnp.pad(\n",
    "                    x, ((0, 0), (0, 0), (pad_width, pad_width)), mode=\"constant\"\n",
    "                )\n",
    "            elif self.pad_mode == \"reflect\":\n",
    "                x = jnp.pad(x, ((0, 0), (0, 0), (pad_width, pad_width)), mode=\"reflect\")\n",
    "\n",
    "        # CQT computation\n",
    "        # Use the efficient convolution implementation\n",
    "        CQT_real = conv1d_efficient(\n",
    "            x,\n",
    "            self.cqt_kernels_real,\n",
    "            stride=self.hop_length,\n",
    "        )\n",
    "        CQT_imag = -conv1d_efficient(\n",
    "            x,\n",
    "            self.cqt_kernels_imag,\n",
    "            stride=self.hop_length,\n",
    "        )\n",
    "\n",
    "        # Apply normalization\n",
    "        if normalization_type == \"librosa\":\n",
    "            CQT_real = CQT_real * jnp.sqrt(self.lengths.reshape(-1, 1))\n",
    "            CQT_imag = CQT_imag * jnp.sqrt(self.lengths.reshape(-1, 1))\n",
    "        elif normalization_type == \"convolutional\":\n",
    "            pass  # No normalization\n",
    "        elif normalization_type == \"wrap\":\n",
    "            CQT_real = CQT_real * 2\n",
    "            CQT_imag = CQT_imag * 2\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"The normalization_type {normalization_type} is not part of our current options.\"\n",
    "            )\n",
    "\n",
    "        # Return the appropriate output format\n",
    "        if output_format == \"Magnitude\":\n",
    "            if not self.trainable:\n",
    "                # Getting CQT Amplitude\n",
    "                CQT = jnp.sqrt(CQT_real**2 + CQT_imag**2)\n",
    "            else:\n",
    "                CQT = jnp.sqrt(CQT_real**2 + CQT_imag**2 + 1e-8)\n",
    "            return CQT\n",
    "\n",
    "        elif output_format == \"Complex\":\n",
    "            return jnp.stack((CQT_real, CQT_imag), axis=-1)\n",
    "\n",
    "        elif output_format == \"Phase\":\n",
    "            phase_real = jnp.cos(jnp.arctan2(CQT_imag, CQT_real))\n",
    "            phase_imag = jnp.sin(jnp.arctan2(CQT_imag, CQT_real))\n",
    "            return jnp.stack((phase_real, phase_imag), axis=-1)\n",
    "\n",
    "\n",
    "# JIT-compiled version for faster execution\n",
    "@partial(jax.jit, static_argnums=(1, 2))\n",
    "def cqt_transform(x, cqt_instance, output_format=None):\n",
    "    \"\"\"JIT-compiled CQT transform function.\"\"\"\n",
    "    return cqt_instance.forward(x, output_format)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
